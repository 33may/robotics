{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541a075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig\n",
    "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
    "from lerobot.policies.smolvla.processor_smolvla import make_smolvla_pre_post_processors\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867576d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"outputs/smolvla_so101_finetune_pickplace_hugginface\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa930a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 11939, ['observation.images.up', 'observation.images.side'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasett_id = \"lerobot/svla_so101_pickplace\"\n",
    "\n",
    "dataset_meta = LeRobotDatasetMetadata(repo_id=datasett_id)\n",
    "\n",
    "dataset_meta.total_episodes, dataset_meta.total_frames, dataset_meta.camera_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(6,)),\n",
       " 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(6,)),\n",
       " 'observation.images.up': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)),\n",
       " 'observation.images.side': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset_to_policy_features(dataset_meta.features)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da384d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = SmolVLAConfig(\n",
    "    input_features=input_features,\n",
    "    output_features=output_features,\n",
    "\n",
    "    n_obs_steps = 1,\n",
    "    chunk_size=50,\n",
    "\n",
    "    freeze_vision_encoder=True,\n",
    "    train_expert_only=True,\n",
    "    train_state_proj=True,\n",
    "\n",
    "    optimizer_lr=1e-4,\n",
    "    optimizer_weight_decay=1e-10,\n",
    "    optimizer_grad_clip_norm=10,\n",
    "\n",
    "    scheduler_warmup_steps=1000,\n",
    "    scheduler_decay_steps=30000,\n",
    "\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the number of VLM layers to 16 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SmolVLAPolicy(\n",
       "  (model): VLAFlowMatching(\n",
       "    (vlm_with_expert): SmolVLMWithExpertModel(\n",
       "      (vlm): SmolVLMForConditionalGeneration(\n",
       "        (model): SmolVLMModel(\n",
       "          (vision_model): SmolVLMVisionTransformer(\n",
       "            (embeddings): SmolVLMVisionEmbeddings(\n",
       "              (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "              (position_embedding): Embedding(1024, 768)\n",
       "            )\n",
       "            (encoder): SmolVLMEncoder(\n",
       "              (layers): ModuleList(\n",
       "                (0-11): 12 x SmolVLMEncoderLayer(\n",
       "                  (self_attn): SmolVLMVisionAttention(\n",
       "                    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "                  (mlp): SmolVLMVisionMLP(\n",
       "                    (activation_fn): GELUTanh()\n",
       "                    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (connector): SmolVLMConnector(\n",
       "            (modality_projection): SmolVLMSimpleMLP(\n",
       "              (proj): Linear(in_features=12288, out_features=960, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (text_model): LlamaModel(\n",
       "            (embed_tokens): Embedding(49280, 960, padding_idx=2)\n",
       "            (layers): ModuleList(\n",
       "              (0-15): 16 x LlamaDecoderLayer(\n",
       "                (self_attn): LlamaAttention(\n",
       "                  (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "                  (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "                  (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "                  (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "                )\n",
       "                (mlp): LlamaMLP(\n",
       "                  (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                  (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                  (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "                  (act_fn): SiLUActivation()\n",
       "                )\n",
       "                (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "                (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "              )\n",
       "            )\n",
       "            (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "        )\n",
       "        (lm_head): Linear(in_features=960, out_features=49280, bias=False)\n",
       "      )\n",
       "      (lm_expert): LlamaModel(\n",
       "        (embed_tokens): None\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (8): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (9): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (10): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (11): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (state_proj): Linear(in_features=32, out_features=960, bias=True)\n",
       "    (action_in_proj): Linear(in_features=32, out_features=720, bias=True)\n",
       "    (action_out_proj): Linear(in_features=720, out_features=32, bias=True)\n",
       "    (action_time_mlp_in): Linear(in_features=1440, out_features=720, bias=True)\n",
       "    (action_time_mlp_out): Linear(in_features=720, out_features=720, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"lerobot/smolvla_base\"\n",
    "\n",
    "policy = SmolVLAPolicy.from_pretrained(\n",
    "    model_id,\n",
    "    config=cfg\n",
    ")\n",
    "\n",
    "preprocessor, postprocessor = make_smolvla_pre_post_processors(cfg, dataset_stats=dataset_meta.stats)\n",
    "\n",
    "policy.train()\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa005bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_delta_timestamps(delta_indices: list[int] | None, fps: int) -> list[float]:\n",
    "    \"\"\"Конвертирует индексы фреймов в временные метки\"\"\"\n",
    "    if delta_indices is None:\n",
    "        return [0]\n",
    "    return [i / fps for i in delta_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196017e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_timestamps = {\n",
    "        \"action\": make_delta_timestamps(\n",
    "            list(range(cfg.chunk_size)),\n",
    "            dataset_meta.fps\n",
    "        ),\n",
    "    }\n",
    "\n",
    "delta_timestamps |= {\n",
    "    k: make_delta_timestamps([0], dataset_meta.fps)\n",
    "    for k in cfg.image_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5ab63",
   "metadata": {},
   "source": [
    "Now we load dataset and configuring data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f96ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeRobotDataset({\n",
       "    Repository ID: 'lerobot/svla_so101_pickplace',\n",
       "    Number of selected episodes: '50',\n",
       "    Number of selected samples: '11939',\n",
       "    Features: '['action', 'observation.state', 'observation.images.up', 'observation.images.side', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
       "})',"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LeRobotDataset(datasett_id, delta_timestamps=delta_timestamps)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5f30c",
   "metadata": {},
   "source": [
    "setup interpreter from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = cfg.get_optimizer_preset().build(policy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory_device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c71815",
   "metadata": {},
   "source": [
    "Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "log_freq = 10\n",
    "save_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "577bf02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tobotics/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for batch in data_loader:\n",
    "        batch = preprocessor(batch)\n",
    "\n",
    "        loss, loss_dict = policy.forward(batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            policy.parameters(),\n",
    "            cfg.optimizer_grad_clip_norm\n",
    "        )\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"Step {step}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "            if loss_dict:\n",
    "                for k, v in loss_dict.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        if v.numel() == 1:\n",
    "                            print(f\"  {k}: {v.item():.4f}\")\n",
    "                        else:\n",
    "                            print(f\"  {k}: mean={v.mean().item():.4f}, shape={v.shape}\")\n",
    "                    else:\n",
    "                        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "        \n",
    "        step += 1\n",
    "        if step >= epochs:\n",
    "            done = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf0c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tobotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
