{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:15.846987Z",
     "start_time": "2025-12-22T20:28:15.843187Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
    "from lerobot.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig\n",
    "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "from lerobot.policies.smolvla.processor_smolvla import make_smolvla_pre_post_processors\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:27:45.465666Z",
     "start_time": "2025-12-22T20:27:45.464090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = Path(\"outputs/smolvla_so101_finetune_pickplace_hugginface\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "1d55be023036d04c",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:43.461517Z",
     "start_time": "2025-12-22T20:28:43.409268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset_id = \"lerobot/svla_so101_pickplace\"\n",
    "\n",
    "dataset_id =\"eternalmay33/pick_place_test\"\n",
    "\n",
    "dataset_meta = LeRobotDatasetMetadata(repo_id=dataset_id)\n",
    "\n",
    "dataset_meta"
   ],
   "id": "3a32d8ef0147e16e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeRobotDatasetMetadata({\n",
       "    Repository ID: 'eternalmay33/pick_place_test',\n",
       "    Total episodes: '39',\n",
       "    Total frames: '8159',\n",
       "    Features: '['action', 'observation.state', 'observation.images.front', 'observation.images.third_person', 'observation.images.gripper', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
       "})',"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:46.556487Z",
     "start_time": "2025-12-22T20:28:46.554529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = dataset_to_policy_features(dataset_meta.features)\n",
    "\n",
    "features"
   ],
   "id": "bb459972230dea08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(6,)),\n",
       " 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(6,)),\n",
       " 'observation.images.front': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)),\n",
       " 'observation.images.third_person': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)),\n",
       " 'observation.images.gripper': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640))}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:49.194362Z",
     "start_time": "2025-12-22T20:28:49.192025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_features = {key : feature for key, feature in features.items() if feature.type is FeatureType.ACTION}\n",
    "input_features = {key : feature for key, feature in features.items() if feature.type is not FeatureType.ACTION}\n",
    "\n",
    "input_features, output_features"
   ],
   "id": "4cd27560e437a700",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(6,)),\n",
       "  'observation.images.front': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)),\n",
       "  'observation.images.third_person': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)),\n",
       "  'observation.images.gripper': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640))},\n",
       " {'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(6,))})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:52.568629Z",
     "start_time": "2025-12-22T20:28:52.566693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = SmolVLAConfig(\n",
    "    input_features=input_features,\n",
    "    output_features=output_features,\n",
    "\n",
    "    n_obs_steps=1,\n",
    "    chunk_size=50,\n",
    "\n",
    "    freeze_vision_encoder=True,\n",
    "    train_expert_only=True,\n",
    "    train_state_proj=True,\n",
    "\n",
    "    optimizer_lr=1e-4,\n",
    "    optimizer_weight_decay=1e-10,\n",
    "    optimizer_grad_clip_norm=10,\n",
    "\n",
    "    scheduler_warmup_steps=1000,\n",
    "    scheduler_decay_steps=30000,\n",
    "\n",
    "    device=device,\n",
    ")"
   ],
   "id": "7bc3cf60d9298ad4",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:15.233048Z",
     "start_time": "2025-12-22T20:28:53.842683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"lerobot/smolvla_base\"\n",
    "\n",
    "policy = SmolVLAPolicy.from_pretrained(\n",
    "    model_id,\n",
    "    config=cfg,\n",
    ")\n",
    "\n",
    "preprocessor, postprocessor = make_smolvla_pre_post_processors(cfg, dataset_stats=dataset_meta.stats)\n",
    "\n",
    "policy.train()\n",
    "policy.to(device)"
   ],
   "id": "3000d92a8e39fc2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the number of VLM layers to 16 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SmolVLAPolicy(\n",
       "  (model): VLAFlowMatching(\n",
       "    (vlm_with_expert): SmolVLMWithExpertModel(\n",
       "      (vlm): SmolVLMForConditionalGeneration(\n",
       "        (model): SmolVLMModel(\n",
       "          (vision_model): SmolVLMVisionTransformer(\n",
       "            (embeddings): SmolVLMVisionEmbeddings(\n",
       "              (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "              (position_embedding): Embedding(1024, 768)\n",
       "            )\n",
       "            (encoder): SmolVLMEncoder(\n",
       "              (layers): ModuleList(\n",
       "                (0-11): 12 x SmolVLMEncoderLayer(\n",
       "                  (self_attn): SmolVLMVisionAttention(\n",
       "                    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "                  (mlp): SmolVLMVisionMLP(\n",
       "                    (activation_fn): GELUTanh()\n",
       "                    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (connector): SmolVLMConnector(\n",
       "            (modality_projection): SmolVLMSimpleMLP(\n",
       "              (proj): Linear(in_features=12288, out_features=960, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (text_model): LlamaModel(\n",
       "            (embed_tokens): Embedding(49280, 960, padding_idx=2)\n",
       "            (layers): ModuleList(\n",
       "              (0-15): 16 x LlamaDecoderLayer(\n",
       "                (self_attn): LlamaAttention(\n",
       "                  (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "                  (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "                  (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "                  (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "                )\n",
       "                (mlp): LlamaMLP(\n",
       "                  (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                  (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                  (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "                  (act_fn): SiLUActivation()\n",
       "                )\n",
       "                (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "                (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "              )\n",
       "            )\n",
       "            (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "        )\n",
       "        (lm_head): Linear(in_features=960, out_features=49280, bias=False)\n",
       "      )\n",
       "      (lm_expert): LlamaModel(\n",
       "        (embed_tokens): None\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (8): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (9): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (10): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (11): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=720, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=720, out_features=960, bias=False)\n",
       "              (k_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (v_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (o_proj): Linear(in_features=960, out_features=720, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=720, out_features=2048, bias=False)\n",
       "              (down_proj): Linear(in_features=2048, out_features=720, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((720,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (state_proj): Linear(in_features=32, out_features=960, bias=True)\n",
       "    (action_in_proj): Linear(in_features=32, out_features=720, bias=True)\n",
       "    (action_out_proj): Linear(in_features=720, out_features=32, bias=True)\n",
       "    (action_time_mlp_in): Linear(in_features=1440, out_features=720, bias=True)\n",
       "    (action_time_mlp_out): Linear(in_features=720, out_features=720, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:17.403440Z",
     "start_time": "2025-12-22T20:29:17.401733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_delta_timestamps(delta_indices: list[int] | None, fps: int) -> list[float]:\n",
    "    \"\"\"Конвертирует индексы фреймов в временные метки\"\"\"\n",
    "    if delta_indices is None:\n",
    "        return [0]\n",
    "    return [i / fps for i in delta_indices]"
   ],
   "id": "683b8d4a945c87c",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:17.729354Z",
     "start_time": "2025-12-22T20:29:17.727592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "delta_timestamps = {\n",
    "    \"action\" : make_delta_timestamps(\n",
    "        list(range(cfg.chunk_size)),\n",
    "        dataset_meta.fps\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "delta_timestamps |= {\n",
    "    k: make_delta_timestamps([-2, -1, 0], dataset_meta.fps)\n",
    "    for k in cfg.image_features\n",
    "}"
   ],
   "id": "d3634a873349a71a",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:18.209774Z",
     "start_time": "2025-12-22T20:29:18.207949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_episodes = dataset_meta.total_episodes\n",
    "\n",
    "episode_idx = np.arange(total_episodes)\n",
    "\n",
    "np.random.shuffle(episode_idx)\n",
    "split_idx = int(0.8 * total_episodes)"
   ],
   "id": "44d99a4585b02f25",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:18.921881Z",
     "start_time": "2025-12-22T20:29:18.716863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_episodes = episode_idx[:split_idx].tolist()\n",
    "val_episodes = episode_idx[split_idx:].tolist()\n",
    "\n",
    "train_dataset = LeRobotDataset(dataset_id, delta_timestamps=delta_timestamps, episodes=train_episodes)\n",
    "\n",
    "val_dataset = LeRobotDataset(dataset_id, delta_timestamps=delta_timestamps, episodes=val_episodes)"
   ],
   "id": "8ec8a7bb37bf39fd",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:19.411989Z",
     "start_time": "2025-12-22T20:29:19.409662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory_device=device,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory_device=device,\n",
    ")"
   ],
   "id": "c9f72ea26dffbda5",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:20.388560Z",
     "start_time": "2025-12-22T20:29:20.385876Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = cfg.get_optimizer_preset().build(policy.parameters())",
   "id": "49a13e6744cdf071",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:20.919123Z",
     "start_time": "2025-12-22T20:29:20.917668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "log_freq = 10\n",
    "val_freq = 20\n",
    "n_val_batches = 10\n",
    "save_freq = 100"
   ],
   "id": "23cbe4b335a12ab2",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:23.703373Z",
     "start_time": "2025-12-22T20:29:21.469994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"smolvla-so101-finetune\",\n",
    "    name=f\"pickplace_chunk{cfg.chunk_size}_lr{cfg.optimizer_lr}\",\n",
    "    config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": cfg.optimizer_lr,\n",
    "        \"chunk_size\": cfg.chunk_size,\n",
    "        \"n_obs_steps\": cfg.n_obs_steps,\n",
    "        \"train_episodes\": len(train_episodes),\n",
    "        \"val_episodes\": len(val_episodes),\n",
    "    }\n",
    ")"
   ],
   "id": "c72c9ad92e6da8ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "6e598e913a0de71ffd78befc2ed658cf"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>▃▅▂▂▂▃▂▂▃▂▂▂▂▁▂▁▁▂▁▂▁▄▂▂▂█▁▁▁▂▂▁▃▂▅▆▂▂▃▄</td></tr><tr><td>train_losses_after_forward</td><td>▃▂▂▂▅▂▃▂▂▂▂▃▂▂▁█▁▂▁▃▁▂▂▁▂▁▁▆▁▁▁▁▁▁▁▂▁▂▂▃</td></tr><tr><td>train_losses_after_rm_padding</td><td>█▆▃▅▂▇▇▄▂▂▃▃▃▂▂▂▂▂▁▁▁▂▄▂▁▂▃▂▂▁▃▃▁▁▁▂▂▁▆▄</td></tr><tr><td>val_loss</td><td>▄█▆▁▄▃▄▃▄</td></tr><tr><td>val_losses_after_forward</td><td>▄█▆▁▄▃▄▃▄</td></tr><tr><td>val_losses_after_rm_padding</td><td>▄█▆▁▄▃▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>step</td><td>198</td></tr><tr><td>train_loss</td><td>0.01777</td></tr><tr><td>train_losses_after_forward</td><td>0.01777</td></tr><tr><td>train_losses_after_rm_padding</td><td>0.01777</td></tr><tr><td>val_loss</td><td>0.02712</td></tr><tr><td>val_losses_after_forward</td><td>0.02712</td></tr><tr><td>val_losses_after_rm_padding</td><td>0.02712</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pickplace_chunk50_lr0.0001</strong> at: <a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/5lyzr0ox' target=\"_blank\">https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/5lyzr0ox</a><br> View project at: <a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune' target=\"_blank\">https://wandb.ai/eternalmay33/smolvla-so101-finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20251222_212545-5lyzr0ox/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "f8096cc0cf566987b38806b089e04185"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/may33/projects/ml_portfolio/robotics/so101_lerobot/wandb/run-20251222_212921-p90at7e4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/p90at7e4' target=\"_blank\">pickplace_chunk50_lr0.0001</a></strong> to <a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune' target=\"_blank\">https://wandb.ai/eternalmay33/smolvla-so101-finetune</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/p90at7e4' target=\"_blank\">https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/p90at7e4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eternalmay33/smolvla-so101-finetune/runs/p90at7e4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f78cdd0d510>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "8x24ansaetj",
   "source": "def validate(policy, val_loader, preprocessor, n_batches, device):\n    \"\"\"Валидация модели на n батчах\"\"\"\n    policy.eval()\n    val_losses = []\n    val_loss_dicts = []\n    \n    with torch.no_grad():\n        for i, batch in enumerate(val_loader):\n            if i >= n_batches:\n                break\n                \n            batch = preprocessor(batch)\n            loss, loss_dict = policy.forward(batch)\n            val_losses.append(loss.item())\n            val_loss_dicts.append(loss_dict)\n    \n    policy.train()\n    \n    # Усредняем метрики\n    avg_val_loss = np.mean(val_losses)\n    avg_loss_dict = {}\n    if val_loss_dicts and val_loss_dicts[0]:\n        for key in val_loss_dicts[0].keys():\n            values = []\n            for d in val_loss_dicts:\n                if key in d:\n                    val = d[key]\n                    if isinstance(val, torch.Tensor):\n                        # Для многомерных тензоров берем mean\n                        values.append(val.mean().item() if val.numel() > 1 else val.item())\n                    else:\n                        values.append(val)\n            avg_loss_dict[f\"val_{key}\"] = np.mean(values)\n    \n    return avg_val_loss, avg_loss_dict",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:29:23.767252Z",
     "start_time": "2025-12-22T20:29:23.764378Z"
    }
   },
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "sxic53mfwyr",
   "source": [
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = preprocessor(batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, loss_dict = policy.forward(batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            policy.parameters(),\n",
    "            cfg.optimizer_grad_clip_norm\n",
    "        )\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        log_dict = {\"train_loss\": loss.item(), \"epoch\": epoch, \"step\": global_step}\n",
    "        if loss_dict:\n",
    "            for k, v in loss_dict.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    log_dict[f\"train_{k}\"] = v.item() if v.numel() == 1 else v.mean().item()\n",
    "                else:\n",
    "                    log_dict[f\"train_{k}\"] = v\n",
    "        wandb.log(log_dict, step=global_step)\n",
    "        \n",
    "        if global_step % log_freq == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Step {global_step} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if global_step % val_freq == 0 and global_step > 0:\n",
    "            print(f\"\\n--- Running validation at step {global_step} ---\")\n",
    "            val_loss, val_loss_dict = validate(policy, val_loader, preprocessor, n_val_batches, device)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            wandb.log({\"val_loss\": val_loss, **val_loss_dict}, step=global_step)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_path = output_dir / \"best_model\"\n",
    "                policy.save_pretrained(best_model_path)\n",
    "                print(f\"Saved best model (val_loss: {val_loss:.4f}) to {best_model_path}\")\n",
    "                wandb.save(str(best_model_path / \"*\"))\n",
    "        \n",
    "        if global_step % save_freq == 0 and global_step > 0:\n",
    "            checkpoint_path = output_dir / f\"checkpoint_step_{global_step}\"\n",
    "            policy.save_pretrained(checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    avg_epoch_loss = np.mean(epoch_losses)\n",
    "    print(f\"\\n=== Epoch {epoch}/{num_epochs} completed | Avg Loss: {avg_epoch_loss:.4f} ===\\n\")\n",
    "    wandb.log({\"epoch_avg_loss\": avg_epoch_loss}, step=global_step)\n",
    "\n",
    "final_path = output_dir / \"final_model\"\n",
    "policy.save_pretrained(final_path)\n",
    "print(f\"\\nTraining completed! Final model saved to {final_path}\")\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:32:52.172547Z",
     "start_time": "2025-12-22T20:29:24.464912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/may33/miniconda3/envs/isaac/lib/python3.11/site-packages/torch/utils/data/dataloader.py:691: UserWarning: 'pin_memory_device' is set but 'pin_memory' argument is not set, then device pinned memory won't be used.please set 'pin_memory' to true, if you need to use the device pin memory\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 | Step 0 | Loss: 0.3662\n",
      "Epoch 0/100 | Step 10 | Loss: 0.1382\n",
      "Epoch 0/100 | Step 20 | Loss: 0.3420\n",
      "\n",
      "--- Running validation at step 20 ---\n",
      "Validation Loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Symlinked 2 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model (val_loss: 0.0899) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Epoch 0/100 | Step 30 | Loss: 0.1143\n",
      "Epoch 0/100 | Step 40 | Loss: 0.1220\n",
      "\n",
      "--- Running validation at step 40 ---\n",
      "Validation Loss: 0.0736\n",
      "Saved best model (val_loss: 0.0736) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Epoch 0/100 | Step 50 | Loss: 0.0923\n",
      "Epoch 0/100 | Step 60 | Loss: 0.0540\n",
      "\n",
      "--- Running validation at step 60 ---\n",
      "Validation Loss: 0.0695\n",
      "Saved best model (val_loss: 0.0695) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Epoch 0/100 | Step 70 | Loss: 0.0980\n",
      "Epoch 0/100 | Step 80 | Loss: 0.0412\n",
      "\n",
      "--- Running validation at step 80 ---\n",
      "Validation Loss: 0.0472\n",
      "Saved best model (val_loss: 0.0472) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Epoch 0/100 | Step 90 | Loss: 0.0588\n",
      "Epoch 0/100 | Step 100 | Loss: 0.0607\n",
      "\n",
      "--- Running validation at step 100 ---\n",
      "Validation Loss: 0.0382\n",
      "Saved best model (val_loss: 0.0382) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Saved checkpoint to outputs/smolvla_so101_finetune_pickplace_hugginface/checkpoint_step_100\n",
      "Epoch 0/100 | Step 110 | Loss: 0.0458\n",
      "Epoch 0/100 | Step 120 | Loss: 0.1047\n",
      "\n",
      "--- Running validation at step 120 ---\n",
      "Validation Loss: 0.0532\n",
      "Epoch 0/100 | Step 130 | Loss: 0.0517\n",
      "Epoch 0/100 | Step 140 | Loss: 0.0649\n",
      "\n",
      "--- Running validation at step 140 ---\n",
      "Validation Loss: 0.0523\n",
      "Epoch 0/100 | Step 150 | Loss: 0.0813\n",
      "Epoch 0/100 | Step 160 | Loss: 0.0383\n",
      "\n",
      "--- Running validation at step 160 ---\n",
      "Validation Loss: 0.0342\n",
      "Saved best model (val_loss: 0.0342) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Epoch 0/100 | Step 170 | Loss: 0.0584\n",
      "Epoch 0/100 | Step 180 | Loss: 0.0305\n",
      "\n",
      "--- Running validation at step 180 ---\n",
      "Validation Loss: 0.0545\n",
      "Epoch 0/100 | Step 190 | Loss: 0.0582\n",
      "Epoch 0/100 | Step 200 | Loss: 0.0304\n",
      "\n",
      "--- Running validation at step 200 ---\n",
      "Validation Loss: 0.0261\n",
      "Saved best model (val_loss: 0.0261) to outputs/smolvla_so101_finetune_pickplace_hugginface/best_model\n",
      "Saved checkpoint to outputs/smolvla_so101_finetune_pickplace_hugginface/checkpoint_step_200\n",
      "Epoch 0/100 | Step 210 | Loss: 0.1118\n",
      "Epoch 0/100 | Step 220 | Loss: 0.0523\n",
      "\n",
      "--- Running validation at step 220 ---\n",
      "Validation Loss: 0.0480\n",
      "Epoch 0/100 | Step 230 | Loss: 0.0494\n",
      "Epoch 0/100 | Step 240 | Loss: 0.0534\n",
      "\n",
      "--- Running validation at step 240 ---\n",
      "Validation Loss: 0.0369\n",
      "Epoch 0/100 | Step 250 | Loss: 0.0722\n",
      "Epoch 0/100 | Step 260 | Loss: 0.0306\n",
      "\n",
      "--- Running validation at step 260 ---\n",
      "Validation Loss: 0.0437\n",
      "Epoch 0/100 | Step 270 | Loss: 0.0363\n",
      "Epoch 0/100 | Step 280 | Loss: 0.0357\n",
      "\n",
      "--- Running validation at step 280 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[66]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_step % val_freq == \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m global_step > \u001B[32m0\u001B[39m:\n\u001B[32m     37\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Running validation at step \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mglobal_step\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m     val_loss, val_loss_dict = \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_val_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mValidation Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     41\u001B[39m     wandb.log({\u001B[33m\"\u001B[39m\u001B[33mval_loss\u001B[39m\u001B[33m\"\u001B[39m: val_loss, **val_loss_dict}, step=global_step)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[65]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mvalidate\u001B[39m\u001B[34m(policy, val_loader, preprocessor, n_batches, device)\u001B[39m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     12\u001B[39m batch = preprocessor(batch)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m loss, loss_dict = \u001B[43mpolicy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m val_losses.append(loss.item())\n\u001B[32m     15\u001B[39m val_loss_dicts.append(loss_dict)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/lerobot/src/lerobot/policies/smolvla/modeling_smolvla.py:368\u001B[39m, in \u001B[36mSmolVLAPolicy.forward\u001B[39m\u001B[34m(self, batch, noise, time)\u001B[39m\n\u001B[32m    366\u001B[39m actions_is_pad = batch.get(\u001B[33m\"\u001B[39m\u001B[33mactions_id_pad\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    367\u001B[39m loss_dict = {}\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m losses = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    369\u001B[39m loss_dict[\u001B[33m\"\u001B[39m\u001B[33mlosses_after_forward\u001B[39m\u001B[33m\"\u001B[39m] = losses.clone()\n\u001B[32m    371\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m actions_is_pad \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/lerobot/src/lerobot/policies/smolvla/modeling_smolvla.py:729\u001B[39m, in \u001B[36mVLAFlowMatching.forward\u001B[39m\u001B[34m(self, images, img_masks, lang_tokens, lang_masks, state, actions, noise, time)\u001B[39m\n\u001B[32m    727\u001B[39m x_t = time_expanded * noise + (\u001B[32m1\u001B[39m - time_expanded) * actions\n\u001B[32m    728\u001B[39m u_t = noise - actions\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m prefix_embs, prefix_pad_masks, prefix_att_masks = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_prefix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    730\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstate\u001B[49m\n\u001B[32m    731\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    732\u001B[39m suffix_embs, suffix_pad_masks, suffix_att_masks = \u001B[38;5;28mself\u001B[39m.embed_suffix(x_t, time)\n\u001B[32m    734\u001B[39m pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/ml_portfolio/robotics/lerobot/src/lerobot/policies/smolvla/modeling_smolvla.py:612\u001B[39m, in \u001B[36mVLAFlowMatching.embed_prefix\u001B[39m\u001B[34m(self, images, img_masks, lang_tokens, lang_masks, state)\u001B[39m\n\u001B[32m    610\u001B[39m \u001B[38;5;66;03m# Normalize image embeddings\u001B[39;00m\n\u001B[32m    611\u001B[39m img_emb_dim = img_emb.shape[-\u001B[32m1\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m612\u001B[39m img_emb = img_emb * \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_emb_dim\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_emb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_emb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    614\u001B[39m bsize, num_img_embs = img_emb.shape[:\u001B[32m2\u001B[39m]\n\u001B[32m    615\u001B[39m img_mask = img_mask[:, \u001B[38;5;28;01mNone\u001B[39;00m].expand(bsize, num_img_embs)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6334fdfd647c27d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
