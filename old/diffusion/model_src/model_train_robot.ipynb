{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0314445227a3baf",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "30e071ec36273394",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c62ccd11679045ea",
   "metadata": {},
   "source": [
    "%env OPENBLAS_NUM_THREADS=16\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "data_path = \"../robomimic/datasets/tool_hang/ph/image_agent.hdf5\"\n",
    "\n",
    "f = h5py.File(data_path, \"r\")\n",
    "\n",
    "data = f[\"data\"]"
   ],
   "id": "3dba3aed726029ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from robotics.model_src.dataset import RobosuiteImageActionDataset, RobosuiteImageActionDatasetMem, normalize_data\n",
    "\n",
    "camera_type = \"agentview\"\n",
    "\n",
    "pred_horizon = 8\n",
    "obs_horizon = 1\n",
    "\n",
    "ds = RobosuiteImageActionDatasetMem(data_path, camera_type, obs_horizon = obs_horizon, pred_horizon = pred_horizon, max_eps_in_ram=100)"
   ],
   "id": "c9de2c082f28b87a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4be688e4f10d7c2",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "val_ratio = 0.2\n",
    "n_total   = len(ds)\n",
    "n_val     = int(n_total * val_ratio)\n",
    "n_train   = n_total - n_val\n",
    "\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "        ds, [n_train, n_val], generator=generator)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=224, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=224, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"batch['image'].shape:\", batch['img_obs'].shape)\n",
    "print(\"batch['act_obs'].shape:\", batch['act_obs'].shape)\n",
    "print(\"batch['act_pred'].shape\", batch['act_pred'].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a0e7af3f0cc470f",
   "metadata": {},
   "source": [
    "from robotics.model_src.diffusion_model import ConditionalUnet1D, ConditionalUnet1DTransformer\n",
    "from robotics.model_src.visual_encoder import CNNVisualEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# visual_encoder = CLIPVisualEncoder().to(device)\n",
    "\n",
    "visual_encoder = CNNVisualEncoder().to(device)\n",
    "\n",
    "vision_feature_dim = visual_encoder.get_output_shape()\n",
    "\n",
    "action_observation_dim = 7\n",
    "\n",
    "obs_dim = vision_feature_dim + action_observation_dim\n",
    "\n",
    "action_dim = 7\n",
    "\n",
    "noise_prediction_net = ConditionalUnet1DTransformer(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon,\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfcf411fb96bd906",
   "metadata": {},
   "source": [
    "image = torch.Tensor(ds[0][\"img_obs\"][None, :obs_horizon, :, :, :]).to(device)\n",
    "act_obs = torch.Tensor(ds[0][\"act_obs\"][None, :obs_horizon, :]).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a28b9062ae5369e",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im = image[0,0, :, :].cpu().numpy()\n",
    "\n",
    "plt.imshow(im.transpose((1, 2, 0)))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "404c3b9fc97655fd",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    image_features = visual_encoder.encode(image.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, act_obs], dim=-1)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim)).to(device)\n",
    "\n",
    "    timestep_tensor = torch.randint(0, 101, (1,), device=device)\n",
    "\n",
    "    noise = noise_prediction_net(\n",
    "        sample=noised_action,\n",
    "        timestep=timestep_tensor,\n",
    "        global_cond=obs.flatten(start_dim=1)\n",
    "    )\n",
    "\n",
    "    denoised_action = noised_action - noise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "accd1216068cb090",
   "metadata": {},
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1cfb8707059725b",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_final_models(visual_encoder, noise_pred_net, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "        },\n",
    "        out_dir / \"model_final.pth\",\n",
    "    )\n",
    "    print(f\"Saved to {out_dir / 'models.pth'}\")\n",
    "\n",
    "def load_final_models(visual_encoder, noise_pred_net, ckpt_path, device=\"cuda\"):\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    visual_encoder.load_state_dict(state[\"visual_encoder\"], strict=True)\n",
    "    noise_pred_net.load_state_dict(state[\"noise_pred_net\"], strict=True)\n",
    "\n",
    "    visual_encoder.to(device).eval()\n",
    "    noise_pred_net.to(device).eval()\n",
    "    print(f\"Loaded weights from {ckpt_path}\")\n",
    "\n",
    "def save_checkpoint(\n",
    "    epoch,\n",
    "    loss,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    out_dir=\"checkpoints\",\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_name = f\"checkpoint_epoch{epoch:03d}_loss{loss:.4f}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"visual_encoder\": visual_encoder.state_dict(),\n",
    "            \"noise_pred_net\": noise_pred_net.state_dict(),\n",
    "            \"ema\": ema.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        out_dir / ckpt_name,\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {out_dir / ckpt_name}\")\n",
    "\n",
    "def load_checkpoint(\n",
    "    ckpt_path,\n",
    "    visual_encoder,\n",
    "    noise_pred_net,\n",
    "    ema,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    map_location=\"cpu\",\n",
    "):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "    noise_pred_net.load_state_dict(ckpt[\"noise_pred_net\"])\n",
    "    ema.load_state_dict(ckpt[\"ema\"])\n",
    "    if optimizer is not None and \"optimizer\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler is not None and \"scheduler\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    print(f\"Checkpoint loaded from {ckpt_path}\")\n",
    "    return ckpt[\"epoch\"], ckpt.get(\"loss\", None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import EMAModel, get_scheduler\n",
    "\n",
    "def forward_loss(nbatch):\n",
    "    nobs  = nbatch['img_obs'][:, :obs_horizon].to(device)\n",
    "    a_obs = nbatch['act_obs'][:, :obs_horizon].to(device)\n",
    "    a_gt  = nbatch['act_pred'].to(device)\n",
    "    B = a_obs.size(0)\n",
    "\n",
    "\n",
    "    image_features = visual_encoder.encode(nobs.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "    image_features = image_features.reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "    obs = torch.cat([image_features, a_obs], dim=-1)\n",
    "\n",
    "    obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "    noise = torch.randn_like(a_gt)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
    "                               (B,), device=device).long()\n",
    "    noisy_a = noise_scheduler.add_noise(a_gt, noise, timesteps)\n",
    "    noise_pred = noise_prediction_net(noisy_a, timesteps, global_cond=obs_cond)\n",
    "    return nn.functional.mse_loss(noise_pred, noise)#%%\n"
   ],
   "id": "4376394fcf62c4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 200\n",
    "\n",
    "# EMA params\n",
    "all_params = list(noise_prediction_net.parameters())\n",
    "ema = EMAModel(parameters=all_params, power=0.75)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=all_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_loader) * num_epochs\n",
    ")\n"
   ],
   "id": "b96abee87c6d98ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_checkpoint(\"../model_src/checkpoints/checkpoint_epoch050_loss0.0398.pth\", visual_encoder, noise_prediction_net, ema, optimizer, lr_scheduler, map_location=\"cuda\")",
   "id": "c7c4a296a598a892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train loop\n",
    "train_hist, val_hist = [], []\n",
    "for epoch_idx in range(52, 150):\n",
    "    epoch_loss_sum = 0.0\n",
    "\n",
    "    for nbatch in train_loader:\n",
    "        # prepare data\n",
    "        nobs = nbatch['img_obs'][:, :obs_horizon].to(device)  # (B, H, state_len)\n",
    "        action_obs = nbatch['act_obs'][:, :obs_horizon].to(device)  # (B, H, 7)\n",
    "        action_pred = nbatch['act_pred'].to(device)  # (B, P, 7)\n",
    "        B = action_obs.size(0)\n",
    "\n",
    "        image_features = visual_encoder.encode(nobs.flatten(start_dim=0, end_dim=1))\n",
    "\n",
    "        image_features = image_features.reshape(*nobs.shape[:2], -1)\n",
    "\n",
    "        obs = torch.cat([image_features, action_obs], dim=-1)\n",
    "\n",
    "        obs_cond = obs.flatten(start_dim=1)  # (B, H*obs_dim)\n",
    "\n",
    "        noise = torch.randn_like(action_pred)\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps,\n",
    "            (B,), device=device).long()\n",
    "\n",
    "        noisy_actions = noise_scheduler.add_noise(action_pred, noise, timesteps)\n",
    "        noise_pred = noise_prediction_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        ema.step(all_params)\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "\n",
    "        avg_train = epoch_loss_sum / len(train_loader)\n",
    "        train_hist.append(avg_train)\n",
    "\n",
    "    # validation\n",
    "    noise_prediction_net.eval()\n",
    "    val_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            val_sum += forward_loss(batch).item()\n",
    "    avg_val = val_sum / len(val_loader)\n",
    "    val_hist.append(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch_idx+1:03d}/{num_epochs} | \"\n",
    "          f\"train {avg_train:.6f} | val {avg_val:.6f}\")\n",
    "\n",
    "    if (epoch_idx + 1) % 10 == 0:\n",
    "        save_checkpoint(\n",
    "            epoch=epoch_idx + 1,\n",
    "            loss=avg_val,\n",
    "            visual_encoder=visual_encoder,\n",
    "            noise_pred_net=noise_prediction_net,\n",
    "            ema=ema,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            out_dir=\"checkpoints\",\n",
    "        )\n",
    "\n",
    "# copy EMA weights for inference\n",
    "ema.copy_to(all_params)"
   ],
   "id": "6ef21157758ee4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ckpt_path = \"./checkpoints/checkpoint_epoch190_loss0.0316.pth\"\n",
    "#\n",
    "# ckpt = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "# visual_encoder.load_state_dict(ckpt[\"visual_encoder\"])\n",
    "# noise_prediction_net.load_state_dict(ckpt[\"noise_pred_net\"])"
   ],
   "id": "21e5a38fb73e0156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f7349f71d96bd5",
   "metadata": {},
   "source": [
    "save_final_models(visual_encoder, noise_prediction_net,\n",
    "                  \"../models/robot_v9_tool_hang_agent_224_e131\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c55255cce7005",
   "metadata": {},
   "source": "load_final_models(visual_encoder, noise_prediction_net, \"../models/robot_v8_tool_hang_agent_224_e40/model_final.pth\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            val_sum += forward_loss(batch).item()\n",
    "\n",
    "val_sum / len(val_loader)"
   ],
   "id": "d8b5982238c9a952",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c7ae7c4c252883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:37:11.735582Z",
     "start_time": "2025-06-21T14:37:11.702483Z"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7de96297d547660",
   "metadata": {},
   "source": [
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.utils.vis_utils import depth_to_rgb\n",
    "from robomimic.envs.env_base import EnvBase, EnvType\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=data_path)\n",
    "env_meta[\"env_kwargs\"][\"reward_shaping\"] = True\n",
    "env_meta[\"env_kwargs\"][\"reward_scale\"]   = 1.0\n",
    "\n",
    "dummy_spec = dict(\n",
    "    obs=dict(\n",
    "        low_dim=[\"robot0_eef_pos\"],\n",
    "        rgb=[\"agentview_image\"]\n",
    "        # rgb=[\"robot0_eye_in_hand_image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ObsUtils.initialize_obs_utils_with_obs_specs(obs_modality_specs=dummy_spec)\n",
    "\n",
    "env = EnvUtils.create_env_from_metadata(env_meta=env_meta, render=True, render_offscreen=True, use_image_obs=True)\n",
    "\n",
    "a = env.reset()\n",
    "\n",
    "from collections import deque\n",
    "obs_deque  = deque(maxlen=obs_horizon)\n",
    "act_deque  = deque(maxlen=obs_horizon)\n",
    "rewards    = []\n",
    "imgs       = []\n",
    "step_idx   = 0\n",
    "\n",
    "max_steps = 500\n",
    "action_horizon  = 4\n",
    "\n",
    "# ─── 6. Main rollout ──────────────────────────────────────────────────────────\n",
    "obs = env.reset()\n",
    "# wrap obs in same format as env.step\n",
    "obs = obs if isinstance(obs, dict) else obs[0]\n",
    "for i in range(obs_deque.maxlen):\n",
    "    obs_deque.append(obs)\n",
    "    act_deque.append(np.zeros(action_dim, dtype=np.float32))\n",
    "\n",
    "pbar = tqdm(total=max_steps)\n",
    "done = False\n",
    "\n",
    "while not done and step_idx < max_steps:\n",
    "    # 6.1 build the image & action history tensor\n",
    "    img_np = np.array([obs_deque[i][camera_type + \"_image\"] for i in range(obs_deque.maxlen)])\n",
    "\n",
    "    img_t   = torch.from_numpy(img_np).float().to(device)\n",
    "\n",
    "    actions_hist = torch.stack(\n",
    "        [torch.from_numpy(a) for a in list(act_deque)],\n",
    "        dim=0\n",
    "    ).to(device)                           # (1, H_a, 7)\n",
    "\n",
    "    # 6.2 compute visual features + conditioning\n",
    "    with torch.no_grad():\n",
    "        img_feat = visual_encoder(img_t)                # (1, C)\n",
    "        obs_cond = torch.cat([img_feat.flatten(start_dim=0).unsqueeze(0) , actions_hist.flatten(start_dim=0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        # 6.3 sample a future action sequence via diffusion\n",
    "        B = 1\n",
    "        pred_actions = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            noise_pred    = noise_prediction_net(pred_actions, t, global_cond=obs_cond)\n",
    "            out           = noise_scheduler.step(noise_pred, t, pred_actions)\n",
    "            pred_actions  = out.prev_sample\n",
    "\n",
    "    pred_actions = pred_actions.cpu().numpy()[0]        # (pred_horizon, 7)\n",
    "\n",
    "    # 6.4 execute the next block of actions\n",
    "    start = obs_horizon\n",
    "    end   = start + action_horizon\n",
    "    action_block = pred_actions[start:end]          # (5, 7)\n",
    "\n",
    "    for act in action_block:\n",
    "        obs, rew, done, info = env.step(act)\n",
    "        obs = obs if isinstance(obs, dict) else obs[0]\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\", height=512, width=512)\n",
    "\n",
    "        obs_deque.append(obs)\n",
    "        act_deque.append(act.astype(np.float32))\n",
    "\n",
    "        rewards.append(rew)\n",
    "        imgs.append(frame)\n",
    "\n",
    "        step_idx += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(reward=float(rew))\n",
    "\n",
    "        if done or step_idx >= max_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ─── 7. Wrap up ───────────────────────────────────────────────────────────────\n",
    "print(f\"Rollout finished: {step_idx} steps, total reward {sum(rewards):.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fe8151cf6b7358b",
   "metadata": {},
   "source": [
    "import imageio\n",
    "\n",
    "video_path = \"test_larger_size_img.mp4\"\n",
    "fps = 24\n",
    "\n",
    "with imageio.get_writer(video_path, fps=fps, codec=\"libx264\") as writer:\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video to {video_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f62615db63e5ccd6",
   "metadata": {},
   "source": "img = img_np[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(img.transpose(1,2,0))",
   "id": "10812269deff5c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5730143767b72223",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
